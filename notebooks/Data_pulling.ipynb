{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8becf076-67f4-47e4-9a12-ed178fd83c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kikis/Repos/reading_comprehension/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: ../data/\n",
      "Dataset loaded successfully\n",
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['repo', 'path', 'func_name', 'original_string', 'language', 'code', 'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url', 'partition', 'summary'],\n",
      "        num_rows: 455243\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|█████████████████████████████████████████████████████████████████████████████████████| 456/456 [00:05<00:00, 77.98ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to save data to: ../data/combined_data.parquet\n",
      "File successfully created at: ../data/combined_data.parquet\n",
      "File size: 598461591 bytes\n",
      "Total number of rows: 455243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "OUTPUT_DIR = \"../data/\"\n",
    "OUTPUT_FILE = \"combined_data.parquet\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    ds = load_dataset(\"Nan-Do/code-search-net-python\")\n",
    "    print(\"Dataset loaded successfully\")\n",
    "    print(f\"Dataset structure: {ds}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# Combine all partitions into a single dataset\n",
    "combined_dataset = ds[next(iter(ds))]  \n",
    "for partition in list(ds.keys())[1:]:\n",
    "    combined_dataset = combined_dataset.concatenate_datasets([ds[partition]])\n",
    "\n",
    "# Save the combined dataset as a single Parquet file\n",
    "output_path = os.path.join(OUTPUT_DIR, OUTPUT_FILE)\n",
    "try:\n",
    "    combined_dataset.to_parquet(output_path)\n",
    "    print(f\"Attempting to save data to: {output_path}\")\n",
    "    \n",
    "    # Verify that the file was created\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"File successfully created at: {output_path}\")\n",
    "        print(f\"File size: {os.path.getsize(output_path)} bytes\")\n",
    "    else:\n",
    "        print(f\"Error: File was not created at {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving to Parquet: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"Total number of rows: {len(combined_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b25becb-dfd7-4f5f-8d7e-f6f2a1ce8cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 455243 entries, 0 to 455242\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   repo              455243 non-null  object\n",
      " 1   path              455243 non-null  object\n",
      " 2   func_name         455243 non-null  object\n",
      " 3   original_string   455243 non-null  object\n",
      " 4   language          455243 non-null  object\n",
      " 5   code              455243 non-null  object\n",
      " 6   code_tokens       455243 non-null  object\n",
      " 7   docstring         455243 non-null  object\n",
      " 8   docstring_tokens  455243 non-null  object\n",
      " 9   sha               455243 non-null  object\n",
      " 10  url               455243 non-null  object\n",
      " 11  partition         455243 non-null  object\n",
      " 12  summary           455243 non-null  object\n",
      "dtypes: object(13)\n",
      "memory usage: 45.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_parquet('../data/combined_data.parquet')\n",
    "\n",
    "\n",
    "print(df.info())\n",
    "df.head()\n",
    "df.describe()\n",
    "\n",
    "df['column_name'].hist()\n",
    "plt.title('Histogram of column_name')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fde020-ae5f-4c4f-a21c-b361f8b0afa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poetry-kernel",
   "language": "python",
   "name": "poetry-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
