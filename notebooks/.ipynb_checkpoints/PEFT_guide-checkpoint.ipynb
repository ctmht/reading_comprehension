{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd978e65a6b2c859"
      },
      "source": [
        "# Parameter-Efficient Fine-Tuning (PEFT) Techniques for Large Language Models (LLMs)\n",
        "\n",
        "In this notebook, we will explore various techniques for fine-tuning large language models in a parameter-efficient way (PEFT). These methods allow us to adapt pre-trained language models to new tasks without updating all the parameters of the model, which is computationally expensive and requires a large amount of data.\n",
        "\n",
        "PEFT strategies are crucial in scenarios where computational resources are limited, or when working with large models like GPT, BERT, or T5. We'll discuss the following techniques:\n",
        "\n",
        "- **LoRA (Low-Rank Adaptation)**\n",
        "- **Prefix Tuning**\n",
        "\n",
        "Let's dive in!"
      ],
      "id": "bd978e65a6b2c859"
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-09-25T14:26:04.145205700Z",
          "start_time": "2024-09-25T14:25:59.415208200Z"
        },
        "id": "87b40f3b6ba08d96"
      },
      "source": [
        "# Installing the necessary libraries\n",
        "!pip install -q transformers datasets\n",
        "# install peft from github\n",
        "!pip install -q git+https://github.com/huggingface/peft"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "87b40f3b6ba08d96"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Low-Rank Adaptation (LoRA)\n",
        "LoRA is another parameter-efficient fine-tuning technique. It reduces the rank of the model's parameter updates to achieve efficient training with fewer resources. This technique works by approximating the parameter updates in a low-dimensional subspace, rather than full-rank matrices."
      ],
      "metadata": {
        "id": "Xij0qPsIR_qe"
      },
      "id": "Xij0qPsIR_qe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data loading and preprocessing\n",
        "\n",
        "In this example, we will use the samsum dataset, which consist of ~16k conversations. Each conversations comes wilt a summary. The objective is to fine-tune a model that is able to generate a summary when forwarded a diaglogue."
      ],
      "metadata": {
        "id": "UUOsCE4Qq81d"
      },
      "id": "UUOsCE4Qq81d"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset from the hub\n",
        "dataset = load_dataset(\"samsum\", trust_remote_code=True)\n",
        "\n",
        "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
        "print(f\"Test dataset size: {len(dataset['test'])}\")\n",
        "\n",
        "print(\"Sample example:\")\n",
        "print(dataset['train'][0])"
      ],
      "metadata": {
        "id": "U3kneC-KrPbP"
      },
      "id": "U3kneC-KrPbP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a model, the text should be converted to machine-readable units, which are the token IDs. This can be done by using a tokenizer.\n",
        "\n",
        "In this example, we'll use a small model from big science for demonstration"
      ],
      "metadata": {
        "id": "63G7rwGJsFn5"
      },
      "id": "63G7rwGJsFn5"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_id=\"bigscience/mt0-small\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "ni5m762fsWhM"
      },
      "id": "ni5m762fsWhM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import concatenate_datasets\n",
        "import numpy as np\n",
        "\n",
        "# Here we tokenize the dialogues, which is the input of our model\n",
        "# The maximum total input sequence length after tokenization.\n",
        "# Sequences longer than this will be truncated, sequences shorter will be padded.\n",
        "tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
        "input_lenghts = [len(x) for x in tokenized_inputs[\"input_ids\"]]\n",
        "# take 85 percentile of max length for better utilization\n",
        "max_source_length = int(np.percentile(input_lenghts, 85))\n",
        "print(f\"Max source length: {max_source_length}\")\n",
        "\n",
        "# Here we tokenize the summary, which should be the output of our model\n",
        "# The maximum total sequence length for target text after tokenization.\n",
        "# Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
        "tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n",
        "target_lenghts = [len(x) for x in tokenized_targets[\"input_ids\"]]\n",
        "# take 90 percentile of max length for better utilization\n",
        "max_target_length = int(np.percentile(target_lenghts, 90))\n",
        "print(f\"Max target length: {max_target_length}\")"
      ],
      "metadata": {
        "id": "RBxGVYiDsoqg"
      },
      "id": "RBxGVYiDsoqg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now will preprocess the data. It's handy to save your preprocessed data to disk for time efficiency"
      ],
      "metadata": {
        "id": "2mi3-WWmt5tU"
      },
      "id": "2mi3-WWmt5tU"
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(sample,padding=\"max_length\"):\n",
        "    # add prefix to the input for t5\n",
        "    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n",
        "\n",
        "    # tokenize inputs which was the dialogue\n",
        "    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
        "\n",
        "    # Tokenize targets with the `text_target` keyword argument, which was the summary\n",
        "    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n",
        "\n",
        "    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
        "    # padding in the loss.\n",
        "    if padding == \"max_length\":\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n",
        "print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")\n",
        "\n",
        "# save datasets to disk for later easy loading\n",
        "tokenized_dataset[\"train\"].save_to_disk(\"data/train\")\n",
        "tokenized_dataset[\"test\"].save_to_disk(\"data/eval\")"
      ],
      "metadata": {
        "id": "XuxQhhoJuEHZ"
      },
      "id": "XuxQhhoJuEHZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model loading and training\n",
        "\n",
        "Now that we have our dataset ready, we can start the fine-tuning process. First we need to load the base model."
      ],
      "metadata": {
        "id": "h3I2oQ3Wu1pK"
      },
      "id": "h3I2oQ3Wu1pK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "# when you're using a big model, you can quantisize the model  to save memory by using its\n",
        "# bit configuration in the parameter setting, that is, 'load_in_4bit=True' or 'load_in_8bit=True'\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id, device_map=\"auto\")"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-09-25T14:29:01.671206800Z",
          "start_time": "2024-09-25T14:29:01.663213200Z"
        },
        "id": "1d012c507baadd74"
      },
      "id": "1d012c507baadd74"
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you want to fine-tune a model, you have to define your fine-tune configuration and wrap the model in a peft-object"
      ],
      "metadata": {
        "id": "KyHjJuRnv_az"
      },
      "id": "KyHjJuRnv_az"
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "\n",
        "# Define LoRA Config\n",
        "lora_config = LoraConfig(\n",
        " r=16,\n",
        " lora_alpha=32,\n",
        " target_modules=[\"q\", \"v\"],\n",
        " lora_dropout=0.05,\n",
        " bias=\"none\",\n",
        " task_type=TaskType.SEQ_2_SEQ_LM\n",
        ")\n",
        "# prepare int-8 model for training when you use a quatizied model\n",
        "# model = repare_model_for_kbit_training(model)\n",
        "\n",
        "# add LoRA adaptor\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "HewDWe5OwJh7"
      },
      "id": "HewDWe5OwJh7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you can see that only 22% of the parameters are being trained, which saves a lot of memory especially for bigger models!\n",
        "\n",
        "Now we create a DataCollator, that will take care of padding the data and create batches"
      ],
      "metadata": {
        "id": "HL732_zXySZ_"
      },
      "id": "HL732_zXySZ_"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "# we want to ignore tokenizer pad token in the loss\n",
        "label_pad_token_id = -100\n",
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=label_pad_token_id,\n",
        "    pad_to_multiple_of=8\n",
        ")"
      ],
      "metadata": {
        "id": "o9buuVpHy1A1"
      },
      "id": "o9buuVpHy1A1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we define the hyperparameters of our training process"
      ],
      "metadata": {
        "id": "Aq3nuffLy4jg"
      },
      "id": "Aq3nuffLy4jg"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
        "\n",
        "output_dir=\"tutorial\"\n",
        "\n",
        "# Define training args\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "\tauto_find_batch_size=True,\n",
        "    learning_rate=1e-3, # higher learning rate\n",
        "    num_train_epochs=5,\n",
        "    logging_dir=f\"{output_dir}/logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=500,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ],
      "metadata": {
        "id": "koMb3LDczCpP"
      },
      "id": "koMb3LDczCpP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can finally train the model"
      ],
      "metadata": {
        "id": "Bx_izJHazNfY"
      },
      "id": "Bx_izJHazNfY"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "M-fGayL5zP6z"
      },
      "id": "M-fGayL5zP6z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model saving and evaluation\n",
        "\n",
        "Make sure you save your model and reload it to check whether everything works accordingly!"
      ],
      "metadata": {
        "id": "AyPPSPXNzW2E"
      },
      "id": "AyPPSPXNzW2E"
    },
    {
      "cell_type": "code",
      "source": [
        "# Save our LoRA model & tokenizer results\n",
        "peft_model_id=\"path_to_trained_model\"\n",
        "trainer.model.save_pretrained(peft_model_id)\n",
        "tokenizer.save_pretrained(peft_model_id)\n",
        "# if you want to save the base model to call\n",
        "# trainer.model.base_model.save_pretrained(peft_model_id)"
      ],
      "metadata": {
        "id": "6TxVRWmXzcOH"
      },
      "id": "6TxVRWmXzcOH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "# Load peft config for pre-trained checkpoint etc.\n",
        "peft_model_id = \"path_to_trained_model\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "\n",
        "# load base LLM model and tokenizer\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path, device_map={\"\":0})\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the Lora model\n",
        "model = PeftModel.from_pretrained(model, peft_model_id, device_map={\"\":0})\n",
        "model.eval()\n",
        "\n",
        "print(\"Peft model loaded\")"
      ],
      "metadata": {
        "id": "_4myQDiDzwHJ"
      },
      "id": "_4myQDiDzwHJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try it with one example from the dataset to see if it works"
      ],
      "metadata": {
        "id": "sp_Hw39jz_1x"
      },
      "id": "sp_Hw39jz_1x"
    },
    {
      "cell_type": "code",
      "source": [
        "# use the first sample of the test set\n",
        "sample = dataset['test'][0]\n",
        "\n",
        "input_ids = tokenizer(sample[\"dialogue\"], return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
        "# with torch.inference_mode():\n",
        "outputs = model.generate(input_ids=input_ids, max_new_tokens=10, do_sample=True, top_p=0.9)\n",
        "print(f\"input sentence: {sample['dialogue']}\\n{'---'* 20}\")\n",
        "\n",
        "print(f\"summary:\\n{tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]}\")"
      ],
      "metadata": {
        "id": "r7rfNdNs0CkH"
      },
      "id": "r7rfNdNs0CkH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it for the LoRA fine-tuning!"
      ],
      "metadata": {
        "id": "Kn9FQZTH0UjO"
      },
      "id": "Kn9FQZTH0UjO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prefix Tuning\n",
        "\n",
        "Prefix tuning enables the model to learn a continuous task-specifc vector which are added to the beginning of the input, the prefix. In this method, only the prefix parameters are optimized, making it easy efficient for training by reducing memory and computational costs by the thousands!\n",
        "\n",
        "### Data loading and preprocessing\n",
        "\n",
        "We will use the financial phrasebank dataset, which contains sentiment labels for financial news sentences.\n"
      ],
      "metadata": {
        "id": "tOwd6jvB0eH5"
      },
      "id": "tOwd6jvB0eH5"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\", trust_remote_code=True)\n",
        "dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
        "dataset[\"validation\"] = dataset[\"test\"]\n",
        "del dataset[\"test\"]\n",
        "\n",
        "classes = dataset[\"train\"].features[\"label\"].names\n",
        "dataset = dataset.map(\n",
        "    lambda x: {\"text_label\": [classes[label] for label in x[\"label\"]]},\n",
        "    batched=True,\n",
        "    num_proc=1,\n",
        ")\n",
        "\n",
        "dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "HgraCODb6GBy"
      },
      "id": "HgraCODb6GBy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we preprocess the data using the tokenizer. In this example, the t5-large model is used."
      ],
      "metadata": {
        "id": "8O27ONuR6gy7"
      },
      "id": "8O27ONuR6gy7"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_id=\"t5-large\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "text_column = \"sentence\"\n",
        "label_column = \"text_label\"\n",
        "max_length = 128\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[text_column]\n",
        "    targets = examples[label_column]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "    labels = tokenizer(targets, max_length=2, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "    labels = labels[\"input_ids\"]\n",
        "    labels[labels == tokenizer.pad_token_id] = -100\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "processed_datasets = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    num_proc=1,\n",
        "    remove_columns=dataset[\"train\"].column_names,\n",
        "    load_from_cache_file=False,\n",
        "    desc=\"Running tokenizer on dataset\",\n",
        ")"
      ],
      "metadata": {
        "id": "tg1OrK2Z6j8G"
      },
      "id": "tg1OrK2Z6j8G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the preprocessing is done, we create a data loader object to forward to the model for training"
      ],
      "metadata": {
        "id": "m7bJ8wV167II"
      },
      "id": "m7bJ8wV167II"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import default_data_collator, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    processed_datasets[\"train\"], shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",
        ")\n",
        "eval_dataloader = DataLoader(processed_datasets[\"validation\"], collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)"
      ],
      "metadata": {
        "id": "9ED6OI3E7BV5"
      },
      "id": "9ED6OI3E7BV5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model loading and training\n",
        "\n",
        "After preparing the data, we can load the model and start initializing the training configuration"
      ],
      "metadata": {
        "id": "NAH3y3Ot7Qxf"
      },
      "id": "NAH3y3Ot7Qxf"
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model, get_peft_model_state_dict, PrefixTuningConfig, TaskType\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import os\n",
        "\n",
        "model_id=\"t5-large\"\n",
        "\n",
        "peft_config = PrefixTuningConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, num_virtual_tokens=20)\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "cLWQ5vdt15JD"
      },
      "id": "cLWQ5vdt15JD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup your optimizer and learning scheduler"
      ],
      "metadata": {
        "id": "pLm9vVYD4w0C"
      },
      "id": "pLm9vVYD4w0C"
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-2\n",
        "num_epochs = 5\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "lr_scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
        ")"
      ],
      "metadata": {
        "id": "6OmTJLWW4mdY"
      },
      "id": "6OmTJLWW4mdY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure the model is set to the right device and start training the model"
      ],
      "metadata": {
        "id": "cYa05XYf41yB"
      },
      "id": "cYa05XYf41yB"
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.detach().float()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "    eval_preds = []\n",
        "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        eval_loss += loss.detach().float()\n",
        "        eval_preds.extend(\n",
        "            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n",
        "        )\n",
        "\n",
        "    eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
        "    eval_ppl = torch.exp(eval_epoch_loss)\n",
        "    train_epoch_loss = total_loss / len(train_dataloader)\n",
        "    train_ppl = torch.exp(train_epoch_loss)\n",
        "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"
      ],
      "metadata": {
        "id": "cBhQ_UaG49nT"
      },
      "id": "cBhQ_UaG49nT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training the model, check how well it performs on the validation set."
      ],
      "metadata": {
        "id": "t_FgeerqNmNM"
      },
      "id": "t_FgeerqNmNM"
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "for pred, true in zip(eval_preds, dataset[\"validation\"][\"text_label\"]):\n",
        "    if pred.strip() == true.strip():\n",
        "        correct += 1\n",
        "    total += 1\n",
        "accuracy = correct / total * 100\n",
        "print(f\"{accuracy=} % on the evaluation dataset\")\n",
        "print(f\"{eval_preds[:10]=}\")\n",
        "print(f\"{dataset['validation']['text_label'][:10]=}\")"
      ],
      "metadata": {
        "id": "jYmYxg1mNraa"
      },
      "id": "jYmYxg1mNraa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model saving and evaluation\n",
        "\n",
        "Be sure to again save your newly trained model and reload it to check if it works properly!\n",
        "\n",
        "You can either push it to the huggingface hub or save it locally."
      ],
      "metadata": {
        "id": "6ujlhpjuNxp3"
      },
      "id": "6ujlhpjuNxp3"
    },
    {
      "cell_type": "code",
      "source": [
        "# pushing it to the huggingface hub\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n",
        "\n",
        "peft_model_id = \"your-name/t5-large_PREFIX_TUNING_SEQ2SEQ\"\n",
        "model.push_to_hub(\"your-name/t5-large_PREFIX_TUNING_SEQ2SEQ\", use_auth_token=True)\n",
        "\n",
        "# after pushing it, you can check whether you can load the PEFT-model\n",
        "peft_model_id = \"your-name/t5-large_PREFIX_TUNING_SEQ2SEQ\"\n",
        "\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "peft_model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
        "peft_model = PeftModel.from_pretrained(model, peft_model_id)"
      ],
      "metadata": {
        "id": "BdBbD1MBOMgQ"
      },
      "id": "BdBbD1MBOMgQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving it locally\n",
        "\n",
        "# save the fine-tuned parametetrs from training\n",
        "model.save_pretrained(\"path_to_save_directory\")\n",
        "\n",
        "# load the base model, which in this case was t5-large\n",
        "base_model = = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "# load the PEFT model from the saved weights in \"path_to_save_directory\"\n",
        "peft_model = PeftModel.from_pretrained(base_model, \"path_to_save_directory\")"
      ],
      "metadata": {
        "id": "YtVpf5fAOtnY"
      },
      "id": "YtVpf5fAOtnY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check with an example whether the model works accordingly"
      ],
      "metadata": {
        "id": "oWrgPWFaP2Al"
      },
      "id": "oWrgPWFaP2Al"
    },
    {
      "cell_type": "code",
      "source": [
        "# put the model in evaluation mode so the weights don't change\n",
        "peft_model.eval()\n",
        "\n",
        "inputs = tokenizer(\n",
        "    \"The Lithuanian beer market made up 14.41 million liters in January , a rise of 0.8 percent from the year-earlier figure , the Lithuanian Brewers ' Association reporting citing the results from its members .\",\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
        "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "v5SSsL3yP6-V"
      },
      "id": "v5SSsL3yP6-V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "552486e74dd58cdc"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we explored several PEFT techniques for fine-tuning large language models. By only modifying a small subset of the model's parameters, these techniques allow us to adapt pre-trained models to new tasks more efficiently, without requiring extensive computational resources or massive amounts of data.\n",
        "- **LoRA** reduces the rank of parameter updates, making training more efficient.\n",
        "- **Prefix Tuning**, optimzing only tthe prefix parameters as only a sequence of continuous task-specific vectors are attached to the beginning of the input\n",
        "\n",
        "These methods enable us to leverage the power of large models while minimizing the computational cost."
      ],
      "id": "552486e74dd58cdc"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AqGpw-4fQqS5"
      },
      "id": "AqGpw-4fQqS5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}