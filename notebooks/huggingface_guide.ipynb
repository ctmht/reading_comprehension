{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebc759c4da84602f",
   "metadata": {
    "collapsed": false,
    "id": "ebc759c4da84602f",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "\n",
    " # Hugging Face Transformers - A Complete Guide\n",
    "\n",
    " This notebook provides a complete guide on how to use Hugging Face Transformers to perform common Natural Language Processing (NLP) tasks such as:\n",
    " - Sentiment Analysis\n",
    " - Text Summarization\n",
    " - Question Answering\n",
    " - Text Translation\n",
    " - Text Generation\n",
    "\n",
    "\n",
    "Additionally, it will demonstrate how to fine-tune a pre-trained model on a custom dataset for specific tasks.\n",
    "\n",
    "# 1. Installing Necessary Libraries\n",
    "Before we can start, we need to install the required Python packages. We will use the Hugging Face `transformers` and `datasets` libraries along with `torch`, which is the backend framework that runs the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c47582c38a640d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T13:11:10.902672300Z",
     "start_time": "2024-09-11T13:11:06.500292200Z"
    },
    "id": "d5c47582c38a640d"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1370508aff367b36",
   "metadata": {
    "collapsed": false,
    "id": "1370508aff367b36",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 2. Using Hugging Face Pipelines\n",
    "\n",
    "Hugging Face provides a high-level abstraction called `pipeline`. The `pipeline` is designed to allow you to quickly apply a model to a task without needing to worry about the underlying details.\n",
    "\n",
    "You can use the `pipeline` function to load a pre-trained model for different tasks such as sentiment analysis, text generation, summarization, etc.\n",
    "\n",
    "Let's start by importing the `pipeline` function from the Hugging Face Transformers library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397273d8a6e1aae7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T13:11:23.664156Z",
     "start_time": "2024-09-11T13:11:10.901679Z"
    },
    "id": "397273d8a6e1aae7"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d241505041a6a8",
   "metadata": {
    "collapsed": false,
    "id": "b1d241505041a6a8",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Task 1: Sentiment Analysis\n",
    "Sentiment Analysis is the task of classifying a given text into positive, negative, or neutral sentiments.\n",
    "\n",
    "In this example, we will use a pre-trained model for sentiment analysis. The `pipeline` will automatically download and load a model that has been pre-trained on a large dataset to perform this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08c573b7d61fca0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.666148400Z"
    },
    "id": "c08c573b7d61fca0"
   },
   "outputs": [],
   "source": [
    "classifier = pipeline('sentiment-analysis')\n",
    "result = classifier(\"I love the Large Language Model course!\")\n",
    "print(f\"Sentiment Analysis Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347a60822e415e0b",
   "metadata": {
    "collapsed": false,
    "id": "347a60822e415e0b",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Task 2: Text Summarization\n",
    "\n",
    "Notice the errors:\n",
    "   - 'Using a pipeline without specifying a model name and revision in production is not recommended.'\n",
    "   - 'FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default.\n",
    "\n",
    "The first error is a warning that suggests specifying the model name and revision when using a pipeline in production. This is important to ensure reproducibility and consistency in your results. Huggingface's standard libraries and versions change frequently, so it's a good practice to specify the model name and revision. This is specified in the cell below, where model is the model name, and revision is the version of the model.\n",
    "\n",
    "`clean_up_tokenization_spaces` removes spaces before punctuations and adds spaces after these punctuations. Only relevant if `add_prefix_space` is `True` in the tokenizer. It makes sure the text is human-readable without odd spacing issues.\n",
    "\n",
    "Furthermore, the gpu is not yet selected, so we need to do that too.\n",
    "\n",
    "Text Summarization is the task of creating a shorter version of a long text while preserving the main content. This can be useful when you need to condense large articles or reports.\n",
    "\n",
    "We'll use the `summarization` pipeline for this task, which leverages models that are fine-tuned specifically for generating summaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699a10d6d117e02",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.669143100Z"
    },
    "id": "699a10d6d117e02"
   },
   "outputs": [],
   "source": [
    "model_name = \"t5-small\" # or gpt-2, facebook/barg-large-cnn, etc.\n",
    "revision = \"main\"  # or a specific commit hash, version, or tag\n",
    "\n",
    "# Check if GPU is available\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model_name, revision=revision, device=device)\n",
    "\n",
    "text = \"Machine learning is the study of computer algorithms that improve automatically through experience. It is seen as a part of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so.\"\n",
    "\n",
    "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
    "print(f\"Summary: {summary[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a77ad3e38ad451e",
   "metadata": {
    "collapsed": false,
    "id": "4a77ad3e38ad451e",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Task 3: Question-Answering\n",
    "\n",
    "Question Answering involves answering a question based on a provided context. This task is useful for systems like chatbots or information retrieval systems where the goal is to answer specific queries from a given body of text.\n",
    "\n",
    "We'll use the `question-answering` pipeline for this task, which requires both a question and a context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfde0988d5c15cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T13:11:23.682116Z",
     "start_time": "2024-09-11T13:11:23.672135200Z"
    },
    "id": "1bfde0988d5c15cc"
   },
   "outputs": [],
   "source": [
    "question_answerer = pipeline(\"question-answering\", device=device)\n",
    "\n",
    "context = \"Machine learning is a subset of artificial intelligence, which involves using statistical techniques to give computer systems the ability to 'learn' from data, without being explicitly programmed.\"\n",
    "\n",
    "question = \"What is machine learning?\"\n",
    "answer = question_answerer(question=question, context=context)\n",
    "print(f\"Answer: {answer['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded074cce4ae141",
   "metadata": {
    "collapsed": false,
    "id": "1ded074cce4ae141",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Task 4: Text Translation\n",
    "\n",
    "Text Translation is the task of converting text from one language to another. Hugging Face provides translation pipelines for a wide range of languages.\n",
    "\n",
    "In this example, we will translate a sentence from English to French using the `translation_en_to_fr` pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c77152427cad9e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.675124600Z"
    },
    "id": "f7c77152427cad9e"
   },
   "outputs": [],
   "source": [
    "translator = pipeline(\"translation_en_to_fr\", device=device)\n",
    "\n",
    "translation = translator(\"Hello, how are you?\")\n",
    "print(f\"Translation: {translation[0]['translation_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478c10561adfcaef",
   "metadata": {
    "collapsed": false,
    "id": "478c10561adfcaef",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Task 5: Text Generation\n",
    "Text Generation involves generating coherent text from a given prompt. Models like GPT-2 are commonly used for this task.\n",
    "\n",
    "We'll use the text-generation pipeline to generate text based on an initial prompt.\n",
    "\n",
    "Let's run the following cell to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6648e9ad31cad9f4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.678118500Z"
    },
    "id": "6648e9ad31cad9f4"
   },
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", device=device)\n",
    "\n",
    "generated_text = generator(\"Artificial intelligence will revolutionize the future of technology\")\n",
    "print(f\"Generated Text: {generated_text[0]['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23025de743370a0f",
   "metadata": {
    "collapsed": false,
    "id": "23025de743370a0f",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 3. Fine-Tuning Pre-trained Models\n",
    "While the pre-trained models provided by Hugging Face are powerful, you may want to fine-tune them for a specific task or dataset.\n",
    "\n",
    "Fine-tuning involves taking a pre-trained model and training it further on your own data. This can improve the model’s performance for specific use cases.\n",
    "\n",
    "For this section, we’ll load the IMDB dataset (which contains movie reviews) and fine-tune a pre-trained model for sentiment classification.\n",
    "\n",
    "### Step 1: Load Dataset\n",
    "We'll use Hugging Face's datasets library to load the IMDB dataset.\n",
    "\n",
    "Datasets from the dataset library often come with pre-defined splits of the data, such as `train` and `test` sets.\n",
    "\n",
    "It is possible to filter or slice datasets to focus on specific subsets of the data, using the `select` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02c5a5d660751f6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.684099300Z"
    },
    "id": "b02c5a5d660751f6"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(100))  # Using a subset for quick fine-tuning\n",
    "test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f2e557f938c88",
   "metadata": {
    "collapsed": false,
    "id": "404f2e557f938c88",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Step 2: Tokenize the Dataset\n",
    "The dataset needs to be tokenized before it can be fed into the model. Tokenization converts the text data into numerical format (tokens) that the model can process.\n",
    "\n",
    "We'll use the `AutoTokenizer` class from HuggingFace to tokenize the data. The `AutoTokenizer` class automatically selects the appropriate tokenizer for the model based on the `model_name`.\n",
    "\n",
    "Tokenization or transformation of the dataset can be done using the `map` method, which applies a function to all the elements of the dataset. This is easily done by defining a function that tokenizes the text data and then applying it to the dataset. When `batched=True`, the function will be applied to batches of data, which can improve performance by applying the function in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bfbf6e30ed6c40",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.687091400Z"
    },
    "id": "76bfbf6e30ed6c40"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # print(examples[\"text\"][0])\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20413a582b139a0e",
   "metadata": {
    "collapsed": false,
    "id": "20413a582b139a0e",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Step 3: Load a Pre-trained Model\n",
    "Now that the data is tokenized, we'll load a pre-trained model that we'll fine-tune for sentiment classification.\n",
    "\n",
    "We'll use distilbert-base-uncased for this task.\n",
    "\n",
    "We need to import `AutoModelForSequenceClassification` for that. The key feature of this class is that it adds a classification head on top of the pre-trained transformer model to allow it to classify sequences into one or more categories (e.g., positive/negative sentiment, spam/ham, etc.). The `from_pretrained` method loads the pre-trained model with the specified configuration. The `num_labels` parameter specifies the number of labels in the classification task (binary in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c217ddcb27c998d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.689086Z"
    },
    "id": "7c217ddcb27c998d"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b466e12de8bfef",
   "metadata": {
    "collapsed": false,
    "id": "8b466e12de8bfef",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Step 4: Set Up the Trainer\n",
    "Hugging Face provides the Trainer class to help with the training and fine-tuning of models. We need to set up the trainer by providing the model, training arguments, and the datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f8aad08895c6e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.691080400Z"
    },
    "id": "314f8aad08895c6e"
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # Output directory\n",
    "    evaluation_strategy=\"epoch\",     # Evaluate after each epoch\n",
    "    learning_rate=2e-5,              # Learning rate\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=8,    # Batch size for evaluation\n",
    "    num_train_epochs=1,              # Number of epochs\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4205eac4d06f8ae5",
   "metadata": {
    "collapsed": false,
    "id": "4205eac4d06f8ae5",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Step 5: Fine-tune the Model\n",
    "Now that the trainer is set up, we can start the fine-tuning process.\n",
    "\n",
    "Run the following cell to fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3125c17af30c4b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.693075300Z"
    },
    "id": "3c3125c17af30c4b"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197d2fb4351c32ea",
   "metadata": {
    "collapsed": false,
    "id": "197d2fb4351c32ea",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Step 6: Evaluate the Model\n",
    "After training, we can evaluate the model’s performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d341d66e17736303",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.695070100Z"
    },
    "id": "d341d66e17736303"
   },
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation Results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1UJ8GcJSPhmt",
   "metadata": {
    "id": "1UJ8GcJSPhmt"
   },
   "source": [
    "### Step 7: Try out model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZTssehqcPd8R",
   "metadata": {
    "id": "ZTssehqcPd8R"
   },
   "outputs": [],
   "source": [
    "input_string = \"I really liked this tutorial!\"\n",
    "\n",
    "# Tokenize the input string\n",
    "inputs = tokenizer(input_string, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Get predictions (logits)\n",
    "with torch.no_grad():  # Disable gradient computation since we're just doing inference\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "predicted_label = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53007a617ec9423",
   "metadata": {
    "collapsed": false,
    "id": "53007a617ec9423",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Step 8. Saving the Fine-tuned Model\n",
    "After training, it is often useful to save the fine-tuned model, so you can use it later without needing to re-train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5c82ccd0913cc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-11T13:11:23.699060Z"
    },
    "id": "a4e5c82ccd0913cc"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./fine-tuned-model\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-model\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
