{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea830f-75d0-4c11-b6cd-87b0ec56c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "import pyarrow as pa\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer,\n",
    "                          DataCollatorForLanguageModeling, Trainer,\n",
    "                          TrainingArguments)\n",
    "\n",
    "# Check and install missing packages\n",
    "required_packages = [\n",
    "    \"datasets\", \"pandas\", \"pyarrow\", \"torch\", \"transformers\"\n",
    "]\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        subprocess.check_call([\"pip\", \"install\", package])\n",
    "\n",
    "\n",
    "class HuggingFaceDataLoader:\n",
    "    \"\"\"\n",
    "    A class for loading datasets from Hugging Face, combining them, and exporting to Parquet or CSV formats.\n",
    "\n",
    "    This class provides functionality to load datasets from Hugging Face, combine multiple partitions\n",
    "    if present, and export the resulting dataset to either Parquet or CSV format.\n",
    "\n",
    "    Attributes:\n",
    "        output_dir (str): The directory where output files will be saved.\n",
    "        dataset: The loaded and combined dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dir: str):\n",
    "        \"\"\"\n",
    "        Initialize the HuggingFaceDataLoader with an output directory.\n",
    "\n",
    "        Args:\n",
    "            output_dir (str): The directory where output files will be saved.\n",
    "        \"\"\"\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def load_dataset(self, link: str):\n",
    "        \"\"\"\n",
    "        Load a dataset from Hugging Face using the provided link.\n",
    "\n",
    "        This method attempts to load the dataset and combines multiple partitions if present.\n",
    "\n",
    "        Args:\n",
    "            link (str): The Hugging Face dataset link.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If there's an error loading the dataset.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.raw_dataset = load_dataset(link)\n",
    "            print(\"Dataset loaded successfully\")\n",
    "            print(f\"Dataset structure: {self.raw_dataset}\")\n",
    "            self._combine_dataset()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _combine_dataset(self):\n",
    "        \"\"\"\n",
    "        Combine multiple partitions of a dataset into a single dataset.\n",
    "\n",
    "        This method is called internally by load_dataset to merge multiple partitions.\n",
    "        \"\"\"\n",
    "        self.dataset = self.raw_dataset[next(iter(self.raw_dataset))]\n",
    "        for partition in list(self.raw_dataset.keys())[1:]:\n",
    "            self.dataset = self.dataset.concatenate_datasets(\n",
    "                [self.raw_dataset[partition]]\n",
    "            )\n",
    "\n",
    "    def output_dataset(self, split: str = None) -> DatasetDict:\n",
    "        \"\"\"\n",
    "        Return the raw dataset as a DatasetDict object, or a specific split as a Dataset object.\n",
    "\n",
    "        Args:\n",
    "            split (str, optional): The name of the split to return. If None, returns the entire DatasetDict.\n",
    "\n",
    "        Returns:\n",
    "            DatasetDict | Dataset: The entire DatasetDict if no split is specified, or a Dataset object for a specific split.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the dataset has not been loaded yet or if the specified split doesn't exist.\n",
    "        \"\"\"\n",
    "        if self.raw_dataset is None:\n",
    "            raise ValueError(\n",
    "                \"Dataset has not been loaded. Please load a dataset first.\"\n",
    "            )\n",
    "\n",
    "        if split is None:\n",
    "            return self.raw_dataset\n",
    "        elif split in self.raw_dataset:\n",
    "            return self.raw_dataset[split]\n",
    "        else:\n",
    "            available_splits = list(self.raw_dataset.keys())\n",
    "            raise ValueError(\n",
    "                f\"Split '{split}' not found. Available splits are: {available_splits}\"\n",
    "            )\n",
    "\n",
    "    def load_local_file(self, file_name: str, chunk_size: int = 100000):\n",
    "        \"\"\"\n",
    "        Load a dataset from a local file.\n",
    "        This method attempts to load the dataset from a local file and sets it to self.raw_dataset.\n",
    "\n",
    "        Args:\n",
    "            file_name (str): The name of the file to load.\n",
    "            chunk_size (int): The number of rows to load at a time for Parquet files.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If there's an error loading the dataset.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            _, extension = os.path.splitext(file_name)\n",
    "            extension = extension.lower()\n",
    "            file_path = os.path.join(self.output_dir, file_name)\n",
    "\n",
    "            if extension == \".csv\":\n",
    "                df = pd.read_csv(file_path)\n",
    "                self.raw_dataset = Dataset.from_pandas(df)\n",
    "            elif extension == \".parquet\":\n",
    "                print(\"Parquet file detected\")\n",
    "                # Use pyarrow to read Parquet file in chunks\n",
    "                parquet_file = pq.ParquetFile(file_path)\n",
    "\n",
    "                # Read and process the file in chunks\n",
    "                chunks = []\n",
    "                for batch in parquet_file.iter_batches(batch_size=chunk_size):\n",
    "                    chunks.append(batch.to_pandas())\n",
    "\n",
    "                # Combine all chunks\n",
    "                df = pd.concat(chunks, ignore_index=True)\n",
    "                self.raw_dataset = Dataset.from_pandas(df)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file type: {extension}\")\n",
    "\n",
    "            print(\"Local dataset loaded successfully\")\n",
    "            print(f\"Dataset structure: {self.raw_dataset}\")\n",
    "            self._combine_dataset()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading local dataset: {e}\")\n",
    "            raise\n",
    "\n",
    "    def output_parquet(self, file_name: str):\n",
    "        \"\"\"\n",
    "        Export the dataset to a Parquet file.\n",
    "\n",
    "        Args:\n",
    "            file_name (str): The name of the output file (without extension).\n",
    "\n",
    "        Raises:\n",
    "            Exception: If there's an error saving the Parquet file.\n",
    "        \"\"\"\n",
    "        output_path_parquet = os.path.join(self.output_dir, file_name + \".parquet\")\n",
    "\n",
    "        try:\n",
    "            self.dataset.to_parquet(output_path_parquet)\n",
    "            print(f\"Attempting to save data to: {output_path_parquet}\")\n",
    "\n",
    "            if os.path.exists(output_path_parquet):\n",
    "                print(f\"Parquet file successfully created at: {output_path_parquet}\")\n",
    "                print(\n",
    "                    f\"Parquet file size: {os.path.getsize(output_path_parquet)} bytes\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Error: Parquet file was not created at {output_path_parquet}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to Parquet: {e}\")\n",
    "            raise\n",
    "\n",
    "    def output_csv(self, file_name: str):\n",
    "        \"\"\"\n",
    "        Export the dataset to a CSV file.\n",
    "\n",
    "        Args:\n",
    "            file_name (str): The name of the output file (without extension).\n",
    "\n",
    "        Raises:\n",
    "            Exception: If there's an error saving the CSV file.\n",
    "        \"\"\"\n",
    "        output_path_csv = os.path.join(self.output_dir, file_name + \".csv\")\n",
    "\n",
    "        try:\n",
    "            self.dataset.to_csv(output_path_csv)\n",
    "            print(f\"Attempting to save data to: {output_path_csv}\")\n",
    "\n",
    "            if os.path.exists(output_path_csv):\n",
    "                print(f\"CSV file successfully created at: {output_path_csv}\")\n",
    "                print(f\"CSV file size: {os.path.getsize(output_path_csv)} bytes\")\n",
    "            else:\n",
    "                print(f\"Error: CSV file was not created at {output_path_csv}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to CSV: {e}\")\n",
    "            raise\n",
    "\n",
    "    def print_dataset(self):\n",
    "        \"\"\"\n",
    "        Print the first few rows of the dataset.\n",
    "\n",
    "        This method is useful for quickly inspecting the loaded dataset.\n",
    "        \"\"\"\n",
    "        print(self.dataset.head())\n",
    "\n",
    "\n",
    "class Llama7B:\n",
    "    \"\"\"Interface for generic Llama7B model with full fine-tuning\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_loader: HuggingFaceDataLoader):\n",
    "        \"\"\"\n",
    "        Initialize the base Llama7B model\n",
    "        \"\"\"\n",
    "        self.base_model = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "        self.dataset = dataset_loader.output_dataset(\"train\")  # pandas DataFrame\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model)\n",
    "        self.model = None\n",
    "\n",
    "    def finetune(self, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Function handling the fine-tuning procedure of the LLM\n",
    "        \"\"\"\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=4,\n",
    "            gradient_accumulation_steps=4,\n",
    "            learning_rate=2e-5,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            save_steps=100,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # Load the model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.base_model,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "\n",
    "        # Prepare the dataset\n",
    "        def formatting_prompts_func(row):\n",
    "            return f\"### Human: {row['instruction']}\\n### Assistant: {row['response']}\"\n",
    "\n",
    "        self.dataset[\"text\"] = self.dataset.apply(formatting_prompts_func, axis=1)\n",
    "\n",
    "        # Tokenize the dataset\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            list(self.dataset[\"text\"]),\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Create labels for causal language modeling\n",
    "        input_ids = tokenized_inputs[\"input_ids\"]\n",
    "        attention_mask = tokenized_inputs[\"attention_mask\"]\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # Create a TensorDataset\n",
    "        train_dataset = TensorDataset(input_ids, attention_mask, labels)\n",
    "\n",
    "        # Define data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer, mlm=False\n",
    "        )\n",
    "\n",
    "        # Initialize the Trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        trainer.save_model(\"./finetuned_model\")\n",
    "\n",
    "    def prompt(self, prompt: str, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Function handling the generation of output\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model has not been fine-tuned or loaded yet.\")\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        outputs = self.model.generate(inputs.input_ids, max_new_tokens=100, **kwargs)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    data_loader = HuggingFaceDataLoader(\"data/\")\n",
    "    data_loader.load_dataset(\"Nan-Do/code-search-net-python\")\n",
    "\n",
    "    model = Llama7B(data_loader)\n",
    "    model.finetune()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
